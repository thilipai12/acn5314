# -*- coding: utf-8 -*-
"""NASNetMobile_final_updated.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Lnt_79JTYkXftnXwbrDOSXJhf8Bn4yCI

##### Copyright 2020
##### Project: Vehicle Identification using Transfer Learning with pretrained CNNs
##### Class: UTD ACN 5314 Spring 2020
##### Authors: Dinakar, Meenakshi, Thilip, Vivek

##### 1. Import required packages and make sure to use the latest Tensorflow version
"""

# Commented out IPython magic to ensure Python compatibility.
import os
import numpy as np
import matplotlib.pyplot as plt
import random
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
import seaborn as sns

# %tensorflow_version 2.x
import tensorflow as tf
import tensorflow_datasets as tfds
print(tf.__version__)

"""##### 2. Download Dataset"""

tfds.disable_progress_bar()
(raw_train, raw_validation, raw_test), metadata = tfds.load(
    'cars196',
    split=['train[:100%]+test[:60%]', 'test[60%:80%]', 'test[80%:100%]'],
    with_info=True,
    as_supervised=True,
)
print(raw_train)
print(raw_validation)
print(raw_test)

"""##### 3. Show first 3 images and labels from training set for sanity check"""

get_label_name = metadata.features['label'].int2str

for image, label in raw_train.take(3):
  plt.figure()
  plt.imshow(image)
  plt.title(get_label_name(label))

"""##### 4. Function to preprocess the dataset to be compatible with the model"""

IMG_SIZE = 224 # All images will be resized to 224x224

def format_example(image, label):
  image = tf.cast(image, tf.float32)
  image = (image/127.5) - 1

  image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))
  return image, label

"""##### 5. Function to prepare training set with data augmentation"""

def aug_format_example(image, label):

  image = tf.cast(image, tf.float32)
  image = (image/127.5) - 1
  image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))
  image = tf.image.random_flip_left_right(image) # random horizontal flip
  image = tf.image.convert_image_dtype(image, tf.float32) # Cast and normalize the image to [0,1]
  image = tf.image.random_brightness(image, max_delta=0.5) # Random brightness
  # random shift
  pad_left  = random.randrange(10,50);
  pad_right = random.randrange(10,50);
  pad_top   = random.randrange(10,50);
  pad_bottom = random.randrange(10,50);
  image = tf.image.pad_to_bounding_box(image, pad_top, pad_left, IMG_SIZE + pad_bottom + pad_top, IMG_SIZE + pad_left + pad_right)
  image = tf.image.crop_to_bounding_box(image, pad_bottom, pad_right, IMG_SIZE, IMG_SIZE)

  return image, label

"""##### 6. Get the final training, validation and test sets. Also, get the data augmented training set."""

train = raw_train.map(format_example)
validation = raw_validation.map(format_example)
test = raw_test.map(format_example)
aug_train = raw_train.map(aug_format_example)

"""##### 7. Show first 3 images of preprocessed training set images"""

for image, label in train.take(3):
  plt.figure()
  plt.imshow(image)
  plt.title(get_label_name(label))

"""##### 8. Show first 3 images of data augmented training set images"""

for image, label in aug_train.take(3):
  plt.figure()
  plt.imshow(image)
  plt.title(get_label_name(label))

"""##### 9. Create batches to train/validate/test and inspect the batch tensor shape"""

BATCH_SIZE = 32
SHUFFLE_BUFFER_SIZE = 1000

train_batches = train.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)
aug_train_batches = aug_train.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)
validation_batches = validation.batch(BATCH_SIZE)
test_batches = test.batch(BATCH_SIZE)

for image_batch, label_batch in train_batches.take(1):
   pass

image_batch.shape

"""##### 10. create base model and set it as non-trainable"""

IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)

base_model = tf.keras.applications.NASNetMobile(input_shape=IMG_SHAPE,
                                               include_top=False,
                                               weights='imagenet')
feature_batch = base_model(image_batch)
print(feature_batch.shape)

base_model.trainable = False
base_model.summary()

"""##### 11. Add classification head and compile the new model"""

global_average_layer = tf.keras.layers.GlobalAveragePooling2D()
feature_batch_average = global_average_layer(feature_batch)
print(feature_batch_average.shape)

prediction_layer = tf.keras.layers.Dense(196)
prediction_batch = prediction_layer(feature_batch_average)
print(prediction_batch.shape)

model = tf.keras.Sequential([
  base_model,
  global_average_layer,
  prediction_layer
])

base_learning_rate = 0.0002
model.compile(optimizer="adam",
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])
model.summary()

"""##### 12. Train the model."""

len(model.trainable_variables)
initial_epochs   =  40
validation_steps = 50

loss0,accuracy0 = model.evaluate(validation_batches, steps = validation_steps)
print("initial loss: {:.2f}".format(loss0))
print("initial accuracy: {:.2f}".format(accuracy0))
history = model.fit(train_batches,
                    epochs=initial_epochs,
                    validation_data=validation_batches
                    )

"""##### 13. Test the model"""

test_steps = validation_steps
test_loss, test_acc = model.evaluate(test_batches, steps = test_steps)

"""##### 14. Plot learning curves"""

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

plt.figure(figsize=(12, 12))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.ylabel('Accuracy')
plt.ylim([0,1])
plt.title('Training and Validation Accuracy')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.ylabel('Cross Entropy')
plt.ylim([0,5.0])
plt.title('Training and Validation Loss')
plt.xlabel('epoch')
plt.show()

"""##### 15. Get the number of layers in base model"""

base_model.trainable = True
print("Number of layers in the base model: ", len(base_model.layers))

"""##### 16. Fine-tune the model

##### 17. Continue to train the model after fine-tuning.
"""

fine_tune_at = 750
for layer in base_model.layers[:fine_tune_at]:
  layer.trainable =  False

model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), #sparse = decimal encoding
              optimizer = tf.keras.optimizers.RMSprop(lr=base_learning_rate/10),
              metrics=['accuracy'])

model.summary()
len(model.trainable_variables)

fine_tune_epochs = 40
total_epochs =  initial_epochs + fine_tune_epochs

history_fine = model.fit(train_batches,
                         epochs=total_epochs,
                         initial_epoch =  history.epoch[-1],
                         validation_data=validation_batches)

"""##### 18. Test the model after fine-tuning"""

test_loss, test_acc = model.evaluate(test_batches, steps = test_steps)

"""##### 19. Plot the learning curves including fine-tuning case"""

acc += history_fine.history['accuracy']
val_acc += history_fine.history['val_accuracy']

loss += history_fine.history['loss']
val_loss += history_fine.history['val_loss']

plt.figure(figsize=(12, 12))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.ylim([0, 1])
plt.plot([initial_epochs-1,initial_epochs-1],
plt.ylim(), label='Start Fine Tuning')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.ylim([0, 5.0])
plt.plot([initial_epochs-1,initial_epochs-1],
plt.ylim(), label='Start Fine Tuning')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.xlabel('epoch')
plt.show()

"""##### 20. Continue training the model with data augmented training set"""

aug_epochs = 40
final_epochs =  total_epochs + aug_epochs

history_aug = model.fit(aug_train_batches,
                         epochs=final_epochs,
                         initial_epoch =  history_fine.epoch[-1],
                         validation_data=validation_batches)

"""##### 21. Test the model after data augmentation"""

test_loss, test_acc = model.evaluate(test_batches, steps = test_steps)

"""##### 22. Plot the learning curves with data augmentation"""

acc += history_aug.history['accuracy']
val_acc += history_aug.history['val_accuracy']

loss += history_aug.history['loss']
val_loss += history_aug.history['val_loss']

plt.figure(figsize=(12, 12))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.ylim([0, 1])
plt.plot([initial_epochs-1,initial_epochs-1],
plt.ylim(), label='Start Fine Tuning')
plt.plot([total_epochs-1,total_epochs-1],
plt.ylim(), label='Start Data Augmentation')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.ylim([0, 5.0])
plt.plot([initial_epochs-1,initial_epochs-1],
plt.ylim(), label='Start Fine Tuning')
plt.plot([total_epochs-1,total_epochs-1],
plt.ylim(), label='Start Data Augmentation')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.xlabel('epoch')
plt.show()

"""##### 23. Compute Precison and Recall"""

y_pred = model.predict_classes(test_batches)

test_cnt = 0
y_pred_name = [0]*2000
y_label = [0]*2000
for image, label in test.take(1608):
  y_pred_name[test_cnt] = get_label_name(y_pred[test_cnt])
  y_label[test_cnt]     = get_label_name(label)
  test_cnt += 1

# precision = tp / (tp + fp)
precision = precision_score(y_label,y_pred_name,average='macro',zero_division=0)
print("Precision:")
print(precision)

# recall = tp / (tp + fn)
recall = recall_score(y_label,y_pred_name,average='macro')
print("Recall:")
print(recall)
